{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 5: PyMC3 and Bayesian Regression\n",
    "\n",
    "## Intro to Quantified Cognition\n",
    "\n",
    "By: Per B. Sederberg, PhD\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/compmem/QuantCog/blob/2021_Spring/notebooks/05_Bayesian_Regression.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson plan\n",
    "\n",
    "- Introduction to PyMC3\n",
    "- Application to BEST\n",
    "- Extension of BEST\n",
    "- Introduce Bayesian regression\n",
    "- Example with simulated data\n",
    "- Robust regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hamiltonian Monte Carlo (HMC)\n",
    "\n",
    "Generating proposals from a random kernel can be *VERY* inefficient.\n",
    "\n",
    "One way is to use the gradients of the posterior. An extremely popular method is the No-U-Turn (NUTS) sampler (Hoffman & Gelman, 2011).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if on Google Colab\n",
    "#!pip install git+https://github.com/arviz-devs/arviz\n",
    "!pip install arvis\n",
    "\n",
    "# to retrieve the dists.py and data files\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/2021_Spring/notebooks/dists.py\n",
    "\n",
    "# if NOT on Google Colab and you need pymc3:\n",
    "#!conda install --yes --prefix {sys.prefix} -c conda-forge pymc3\n",
    "\n",
    "# you may also need graphviz\n",
    "#conda install -c conda-forge python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import pandas as pd               # efficient tables\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "from scipy import stats\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "import dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data that may or may not be significantly different from zero\n",
    "A = dists.normal(mean=0.3, std=0.5).rvs(10)\n",
    "\n",
    "# plot it\n",
    "plt.hist(A, bins='auto', density=True);\n",
    "\n",
    "# do a quick t-test\n",
    "stats.ttest_1samp(A, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    mu = pm.Normal('mu', A.mean(), A.std()*2.0)\n",
    "    sd = pm.Uniform('sd', lower=0.01, upper=10.0)\n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    \n",
    "    # build the model\n",
    "    #lam = data_std**-2.\n",
    "    data = pm.StudentT('data', nu=nu, mu=mu, sd=sd, observed=A)\n",
    "    \n",
    "    # set up some deterministic vars to keep\n",
    "    effect_size = pm.Deterministic('effect_size', mu/sd)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['mu', 'sd', 'df_minus_one', 'effect_size']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['mu', 'sd', 'effect_size'],\n",
    "                  ref_val=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent BEST\n",
    "\n",
    "Let's extend the example to independent samples!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate some data that may or may not be significantly different from each other\n",
    "A = dists.normal(mean=0.2, std=0.5).rvs(10)\n",
    "B = dists.normal(mean=0.4, std=1.0).rvs(12)\n",
    "\n",
    "# plot it\n",
    "plt.hist(A, bins='auto', alpha=0.3);\n",
    "plt.hist(B, bins='auto', alpha=0.3);\n",
    "\n",
    "# do a quick t-test\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first get overall mean and std\n",
    "overall_mean = np.append(A, B).mean()\n",
    "overall_std = np.append(A, B).std()\n",
    "print('overall_mean:', overall_mean)\n",
    "print('ovearll_std:', overall_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the half Cauchy prior\n",
    "x = np.linspace(0, 20, 100)\n",
    "plt.plot(x, dists.halfcauchy(scale=2).pdf(x))\n",
    "\n",
    "plt.plot(x, dists.halfcauchy(scale=5).pdf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a model\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    mu_A = pm.Normal('mu_A', A.mean(), A.std()*2.0)\n",
    "    sd_A = pm.HalfCauchy('sd_A', 5)\n",
    "    \n",
    "    mu_B = pm.Normal('mu_B', B.mean(), B.std()*2.0)\n",
    "    sd_B = pm.HalfCauchy('sd_B', 5)\n",
    "    \n",
    "    \n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    \n",
    "    # build the model\n",
    "    #lam = data_std**-2.\n",
    "    data_A = pm.StudentT('data_A', mu=mu_A, sd=sd_A, nu=nu, observed=A)\n",
    "    data_B = pm.StudentT('data_B', mu=mu_B, sd=sd_B, nu=nu, observed=B)\n",
    "    \n",
    "    # set up some deterministic vars to keep\n",
    "    diff_of_means = pm.Deterministic('difference of means', mu_A - mu_B)\n",
    "    diff_of_stds = pm.Deterministic('difference of stds', sd_A - sd_B)\n",
    "    effect_size = pm.Deterministic('effect size',\n",
    "                                   diff_of_means / np.sqrt((sd_A**2 + sd_B**2) / 2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['mu_A', 'mu_B', 'sd_A', 'sd_B', 'df_minus_one', 'effect size']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['difference of means','difference of stds', 'effect size'],\n",
    "                  ref_val=0.0);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "One of the most common and flexible statistical approaches.\n",
    "\n",
    "Involves building a model that can predict the dependent data ($y$) based on different combinations of independent data ($x$):\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate some data with a linear trend\n",
    "nsamples = 100\n",
    "true_slope = 0.75\n",
    "true_intercept = 1.0\n",
    "true_sigma = 0.5\n",
    "\n",
    "# uniform sampling over x\n",
    "x = dists.uniform(0, 1).rvs(nsamples)\n",
    "\n",
    "# apply noise to linear model\n",
    "y_true = true_intercept + true_slope*x \n",
    "y = y_true + dists.normal(mean=0.0, std=true_sigma).rvs(nsamples)\n",
    "\n",
    "# set the data\n",
    "data = pd.DataFrame(dict(x=x, y=y))\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, y_true, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a standard linear model\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    intercept = pm.Normal('intercept', 0, 20)\n",
    "    slope = pm.Normal('slope', 0, 20)\n",
    "    sigma = pm.HalfCauchy('sigma', 10)\n",
    "    \n",
    "    # calculate the line\n",
    "    mu = intercept + slope * x\n",
    "    \n",
    "    # combine them into a linear function for the likelihood\n",
    "    likelihood = pm.Normal('y', mu=mu, \n",
    "                           sd=sigma, observed=y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# sample the posterior\n",
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['intercept', 'slope', 'sigma'],\n",
    "                  ref_val=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with outliers\n",
    "\n",
    "Sometimes data can be messy. You can either assume every observation affects the statistical inference similarly, or you can try and downplay the effect of potential outliers.\n",
    "\n",
    "This approach is also known as robust regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's add in some outliers!\n",
    "x_out = np.append(x, [.08, .1, .15, .3])\n",
    "y_out = np.append(y, [5.9, 3.54, 4.1, 3.2])\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_out, y_out, 'o')\n",
    "plt.plot(x, y_true, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a linear model with Gaussian noise\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    intercept = pm.Normal('intercept', 0, 20)\n",
    "    slope = pm.Normal('slope', 0, 20)\n",
    "    sigma = pm.HalfCauchy('sigma', 10)\n",
    "    \n",
    "    # combine them into a linear function for the likelihood\n",
    "    likelihood = pm.Normal('y_out', mu=intercept + slope * x_out, \n",
    "                           sd=sigma, observed=y_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['intercept', 'slope', 'sigma'],\n",
    "                  ref_val=[true_intercept, 0.0, true_sigma]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's check with the posterior predictives\n",
    "lm = lambda x, samples: samples['intercept'] + x*samples['slope']\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_out, y_out, 'o')\n",
    "plt.plot(x, y_true, '-')\n",
    "\n",
    "pm.plot_posterior_predictive_glm(trace, eval=np.linspace(0, 1, 100), \n",
    "                                 lm=lm, samples=200, color=\"green\", alpha=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Can we fix it?\n",
    "# define a model\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    intercept = pm.Normal('intercept', 0, 20)\n",
    "    slope = pm.Normal('slope', 0, 20)\n",
    "    sigma = pm.HalfCauchy('sigma', 10)\n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    \n",
    "    # combine them into a robust linear function for the likelihood\n",
    "    likelihood = pm.StudentT('y_out', mu=intercept + slope * x_out, \n",
    "                             sd=sigma, nu=nu, observed=y_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['intercept', 'slope', 'sigma', 'df_minus_one'],\n",
    "                  ref_val=[true_intercept, 0.0, true_sigma, 0.0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's check with the posterior predictives\n",
    "lm = lambda x, samples: samples['intercept'] + x*samples['slope']\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_out, y_out, 'o')\n",
    "plt.plot(x, y_true, '-')\n",
    "\n",
    "pm.plot_posterior_predictive_glm(trace, eval=np.linspace(0, 1, 100), \n",
    "                                 lm=lm, samples=200, color=\"green\", alpha=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment before next class\n",
    "\n",
    "- I'm finally grading old assignments and posting new assignments.\n",
    "- Look for the assignments posted to the assignments folder and on UVACollab\n",
    "\n",
    "### See you next week!!!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
